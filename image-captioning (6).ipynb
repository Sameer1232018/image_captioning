{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T14:47:26.720747Z","iopub.execute_input":"2023-09-17T14:47:26.721422Z","iopub.status.idle":"2023-09-17T14:47:39.554129Z","shell.execute_reply.started":"2023-09-17T14:47:26.721387Z","shell.execute_reply":"2023-09-17T14:47:39.553005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\n\nplt.rcParams['font.size'] = 12\nsns.set_style(\"dark\")\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:47:52.088225Z","iopub.execute_input":"2023-09-17T14:47:52.088670Z","iopub.status.idle":"2023-09-17T14:48:03.014207Z","shell.execute_reply.started":"2023-09-17T14:47:52.088633Z","shell.execute_reply":"2023-09-17T14:48:03.012861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = '/kaggle/input/flickr8kimagescaptions/flickr8k/images'","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:06.154442Z","iopub.execute_input":"2023-09-17T14:48:06.155459Z","iopub.status.idle":"2023-09-17T14:48:06.162683Z","shell.execute_reply.started":"2023-09-17T14:48:06.155420Z","shell.execute_reply":"2023-09-17T14:48:06.159600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:07.251239Z","iopub.execute_input":"2023-09-17T14:48:07.251633Z","iopub.status.idle":"2023-09-17T14:48:07.375112Z","shell.execute_reply.started":"2023-09-17T14:48:07.251603Z","shell.execute_reply":"2023-09-17T14:48:07.373872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    \n    return img\n\ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20 , 20))\n    n = 0\n    for i in range(15):\n        n+=1\n        plt.subplot(5 , 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"/kaggle/input/flickr8kimagescaptions/flickr8k/images/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:08.174871Z","iopub.execute_input":"2023-09-17T14:48:08.175589Z","iopub.status.idle":"2023-09-17T14:48:08.183856Z","shell.execute_reply.started":"2023-09-17T14:48:08.175554Z","shell.execute_reply":"2023-09-17T14:48:08.182754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images(data.sample(15))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:10.370367Z","iopub.execute_input":"2023-09-17T14:48:10.371078Z","iopub.status.idle":"2023-09-17T14:48:13.697317Z","shell.execute_reply.started":"2023-09-17T14:48:10.371045Z","shell.execute_reply":"2023-09-17T14:48:13.696441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:25.747522Z","iopub.execute_input":"2023-09-17T14:48:25.747885Z","iopub.status.idle":"2023-09-17T14:48:25.756870Z","shell.execute_reply.started":"2023-09-17T14:48:25.747855Z","shell.execute_reply":"2023-09-17T14:48:25.755614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessed Text","metadata":{}},{"cell_type":"code","source":"data = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:32.618400Z","iopub.execute_input":"2023-09-17T14:48:32.618744Z","iopub.status.idle":"2023-09-17T14:48:32.831751Z","shell.execute_reply.started":"2023-09-17T14:48:32.618717Z","shell.execute_reply":"2023-09-17T14:48:32.830834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:35.986658Z","iopub.execute_input":"2023-09-17T14:48:35.987073Z","iopub.status.idle":"2023-09-17T14:48:37.044302Z","shell.execute_reply.started":"2023-09-17T14:48:35.987041Z","shell.execute_reply":"2023-09-17T14:48:37.043399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    img = np.expand_dims(img,axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature","metadata":{"execution":{"iopub.status.busy":"2023-09-17T14:48:38.580552Z","iopub.execute_input":"2023-09-17T14:48:38.580896Z","iopub.status.idle":"2023-09-17T15:03:37.533041Z","shell.execute_reply.started":"2023-09-17T14:48:38.580869Z","shell.execute_reply":"2023-09-17T15:03:37.531974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataGenerator(Sequence):\n    \n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n                 vocab_size, max_length, features,shuffle=True):\n    \n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __len__(self):\n        return self.n // self.batch_size\n    \n    def __getitem__(self,index):\n    \n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)        \n        return (X1, X2), y\n    \n    def __get_data(self,batch):\n        \n        X1, X2, y = list(), list(), list()\n        \n        images = batch[self.X_col].tolist()\n           \n        for image in images:\n            feature = self.features[image][0]\n            \n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            \n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                \n        return X1, X2, y","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:46.741815Z","iopub.execute_input":"2023-09-17T15:03:46.742198Z","iopub.status.idle":"2023-09-17T15:03:46.757108Z","shell.execute_reply.started":"2023-09-17T15:03:46.742168Z","shell.execute_reply":"2023-09-17T15:03:46.756092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"input1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1,input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy',optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:49.005856Z","iopub.execute_input":"2023-09-17T15:03:49.006284Z","iopub.status.idle":"2023-09-17T15:03:49.403219Z","shell.execute_reply.started":"2023-09-17T15:03:49.006253Z","shell.execute_reply":"2023-09-17T15:03:49.402288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:49.947714Z","iopub.execute_input":"2023-09-17T15:03:49.948644Z","iopub.status.idle":"2023-09-17T15:03:49.954067Z","shell.execute_reply.started":"2023-09-17T15:03:49.948596Z","shell.execute_reply":"2023-09-17T15:03:49.952642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Modification","metadata":{}},{"cell_type":"code","source":"plot_model(caption_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:53.302505Z","iopub.execute_input":"2023-09-17T15:03:53.303171Z","iopub.status.idle":"2023-09-17T15:03:53.567657Z","shell.execute_reply.started":"2023-09-17T15:03:53.303136Z","shell.execute_reply":"2023-09-17T15:03:53.566835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:54.992654Z","iopub.execute_input":"2023-09-17T15:03:54.993063Z","iopub.status.idle":"2023-09-17T15:03:55.023159Z","shell.execute_reply.started":"2023-09-17T15:03:54.993032Z","shell.execute_reply":"2023-09-17T15:03:55.022472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\nvalidation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:56.225523Z","iopub.execute_input":"2023-09-17T15:03:56.225873Z","iopub.status.idle":"2023-09-17T15:03:56.234000Z","shell.execute_reply.started":"2023-09-17T15:03:56.225845Z","shell.execute_reply":"2023-09-17T15:03:56.232835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"model.h5\"\ncheckpoint = ModelCheckpoint(model_name,\n                            monitor=\"val_loss\",\n                            mode=\"min\",\n                            save_best_only = True,\n                            verbose=1)\n\nearlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00000001)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:03:57.481591Z","iopub.execute_input":"2023-09-17T15:03:57.482130Z","iopub.status.idle":"2023-09-17T15:03:57.488763Z","shell.execute_reply.started":"2023-09-17T15:03:57.482098Z","shell.execute_reply":"2023-09-17T15:03:57.487550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = caption_model.fit(\n        train_generator,\n        epochs=15,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:05:01.342329Z","iopub.execute_input":"2023-09-17T15:05:01.342689Z","iopub.status.idle":"2023-09-17T15:22:16.502265Z","shell.execute_reply.started":"2023-09-17T15:05:01.342660Z","shell.execute_reply":"2023-09-17T15:22:16.501135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.savefig('model_loss_plot.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:33:41.528318Z","iopub.execute_input":"2023-09-17T15:33:41.528683Z","iopub.status.idle":"2023-09-17T15:33:42.233955Z","shell.execute_reply.started":"2023-09-17T15:33:41.528653Z","shell.execute_reply":"2023-09-17T15:33:42.232867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def idx_to_word(integer,tokenizer):\n    \n    for word, index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:23:17.449644Z","iopub.execute_input":"2023-09-17T15:23:17.450048Z","iopub.status.idle":"2023-09-17T15:23:17.456032Z","shell.execute_reply.started":"2023-09-17T15:23:17.450016Z","shell.execute_reply":"2023-09-17T15:23:17.454842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef predict_caption(model, image, tokenizer, max_length, features):\n    \n    feature = features[image]\n    in_text = \"startseq\"\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], max_length)\n\n        y_pred = model.predict([feature,sequence])\n        y_pred = np.argmax(y_pred)\n        \n        word = idx_to_word(y_pred, tokenizer)\n        \n        if word is None:\n            break\n            \n        in_text+= \" \" + word\n        \n        if word == 'endseq':\n            break\n            \n    return in_text ","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:23:20.978821Z","iopub.execute_input":"2023-09-17T15:23:20.979525Z","iopub.status.idle":"2023-09-17T15:23:20.986626Z","shell.execute_reply.started":"2023-09-17T15:23:20.979492Z","shell.execute_reply":"2023-09-17T15:23:20.985585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15 Random Samples for caption prediction","metadata":{}},{"cell_type":"code","source":"samples = test.sample(15)\nsamples.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:23:25.125014Z","iopub.execute_input":"2023-09-17T15:23:25.125724Z","iopub.status.idle":"2023-09-17T15:23:25.131662Z","shell.execute_reply.started":"2023-09-17T15:23:25.125687Z","shell.execute_reply":"2023-09-17T15:23:25.130630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,record in samples.iterrows():\n\n    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n    img = img_to_array(img)\n    img = img/255.\n    \n    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n    samples.loc[index,'caption'] = caption","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:23:27.598580Z","iopub.execute_input":"2023-09-17T15:23:27.599277Z","iopub.status.idle":"2023-09-17T15:23:37.890218Z","shell.execute_reply.started":"2023-09-17T15:23:27.599244Z","shell.execute_reply":"2023-09-17T15:23:37.889226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images(samples)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:26:05.405738Z","iopub.execute_input":"2023-09-17T15:26:05.406227Z","iopub.status.idle":"2023-09-17T15:26:08.502192Z","shell.execute_reply.started":"2023-09-17T15:26:05.406187Z","shell.execute_reply":"2023-09-17T15:26:08.501335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}